{
    "learning_rate": 0.0003,
    "batch_size": 32,
    "num_epochs": 4,
    "gradient_accumulation_steps": 8,
    "warmup_ratio": 0.201,
    "model_name": "llama-3-8b-instruct",
    "max_seq_length": 2048,
    "fp16": 1,
    "dataset_path": "data/final_datasets/12345.json"
}